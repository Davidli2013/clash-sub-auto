name: Auto Update HK SG JP Nodes

on:
  schedule:
    - cron: '0 8-23/2 * * *'  # 每2小时自动更新
  workflow_dispatch:

jobs:
  update-region-nodes:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install pyyaml requests

      - name: Download and filter nodes (HK:100个, SG/JP:30个, 保底30个)
        run: |
          python3 << 'EOF'
          import os
          import yaml
          import random
          import requests
          from typing import List, Dict

          # ========== 核心配置 ==========
          CONFIG_FILE = "config.yaml"
          TIMEOUT = 15
          MIN_NODES = 30  # 所有地区保底30个
          # 各地区最大节点数配置
          MAX_NODES_CONFIG = {
              "hk": 100,  # 香港保留100个
              "sg": 30,   # 新加坡保留30个
              "jp": 30    # 日本保留30个
          }
          # 替换为你的Cloudflare Worker地址
          CF_WORKERS_API = "https://yellow-tree-130b.lijinlonghk666.workers.dev"

          # 地区配置
          REGION_CONFIG = {
              "hk": {
                  "filename": "sub_hk.yaml",
                  "keywords": ["hk", "hongkong", "香港", "HK", "HongKong"],
                  "prefix": "HK-"
              },
              "sg": {
                  "filename": "sub_sg.yaml",
                  "keywords": ["sg", "singapore", "新加坡", "SG", "Singapore"],
                  "prefix": "SG-"
              },
              "jp": {
                  "filename": "sub_jp.yaml",
                  "keywords": ["jp", "japan", "日本", "JP", "Japan", "东京", "大阪"],
                  "prefix": "JP-"
              }
          }

          ALLOWED_PROTOCOLS = ["trojan", "vmess", "vless", "shadowsocks", "ssr"]

          class ProxyNode:
              def __init__(self, d, prefix):
                  self.name = f"{prefix}{d.get('name', f'Node{random.randint(1000,9999)}')}"
                  self.type = d.get("type", "")
                  self.server = d.get("server", "")
                  self.port = d.get("port", 443)
                  self.password = d.get("password", "")
                  self.sni = d.get("sni", "")
                  self.skip_cert_verify = d.get("skip-cert-verify", True)
                  self.udp = d.get("udp", True)
                  self.network = d.get("network", "tcp")
                  self.uuid = d.get("uuid", "")
                  self.cipher = d.get("cipher", "")
                  self.latency = 9999.0
                  self.available = False

              def to_dict(self):
                  b = {
                      "name": self.name,
                      "type": self.type,
                      "server": self.server,
                      "port": self.port,
                      "skip-cert-verify": self.skip_cert_verify,
                      "udp": self.udp
                  }
                  if self.type == "trojan":
                      b["password"] = self.password
                      if self.sni:
                          b["sni"] = self.sni
                  elif self.type in ["vmess", "vless"]:
                      b["uuid"] = self.uuid
                      if self.sni:
                          b["servername"] = self.sni
                  elif self.type in ["shadowsocks", "ssr"]:
                      b["password"] = self.password
                      b["cipher"] = self.cipher
                  if self.network != "tcp":
                      b["network"] = self.network
                  return b

              def get_unique_key(self):
                  return f"{self.type}_{self.server}_{self.port}_{self.password[:8]}"

              def test_latency_cf(self):
                  """调用Cloudflare Workers测速（内地视角）"""
                  try:
                      resp = requests.post(
                          CF_WORKERS_API,
                          json={"server": self.server, "port": self.port},
                          timeout=TIMEOUT
                      )
                      if resp.status_code == 200:
                          self.latency = resp.json().get("latency", 9999.0)
                          self.available = True
                  except Exception as e:
                      print(f"测速失败 {self.name}: {e}")

          def load_existing_nodes(filename: str) -> List[ProxyNode]:
              existing_nodes = []
              if not os.path.exists(filename):
                  return existing_nodes
              try:
                  with open(filename, 'r', encoding='utf-8') as f:
                      config = yaml.safe_load(f)
                  if not config or "proxies" not in config:
                      return existing_nodes
                  for p in config["proxies"]:
                      if p.get("type") in ALLOWED_PROTOCOLS:
                          prefix = ""
                          for code, cfg in REGION_CONFIG.items():
                              if p["name"].startswith(cfg["prefix"]):
                                  prefix = cfg["prefix"]
                                  break
                          existing_nodes.append(ProxyNode(p, prefix))
              except Exception as e:
                  print(f"读取历史节点失败: {e}")
              return existing_nodes

          def load_subs():
              if not os.path.exists(CONFIG_FILE):
                  print(f"配置文件 {CONFIG_FILE} 不存在！")
                  return []
              with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                  return yaml.safe_load(f).get("subs", [])

          def get_content(url):
              try:
                  r = requests.get(url, timeout=TIMEOUT)
                  r.raise_for_status()
                  return r.text
              except Exception as e:
                  print(f"下载订阅失败 {url}: {e}")
                  return ""

          def parse_proxies(txt):
              try:
                  d = yaml.safe_load(txt)
                  if isinstance(d, dict) and "proxies" in d:
                      return d["proxies"]
                  elif isinstance(d, list):
                      return d
              except Exception as e:
                  print(f"解析节点失败: {e}")
              return []

          def filter_region(proxies, kws):
              res = []
              for p in proxies:
                  if not isinstance(p, dict):
                      continue
                  if p.get("type") not in ALLOWED_PROTOCOLS:
                      continue
                  n = str(p.get("name", "")).lower()
                  s = str(p.get("server", "")).lower()
                  if any(kw.lower() in n or kw.lower() in s for kw in kws):
                      res.append(p)
              return res

          def merge_nodes(existing: List[ProxyNode], new: List[ProxyNode], max_nodes: int) -> List[ProxyNode]:
              """合并节点：去重 + 按延迟排序 + 按地区限制最大数 + 保底30个"""
              node_dict = {n.get_unique_key(): n for n in existing}
              # 添加新节点（不重复）
              for n in new:
                  key = n.get_unique_key()
                  if key not in node_dict:
                      node_dict[key] = n
              # 按延迟排序
              merged = list(node_dict.values())
              merged.sort(key=lambda x: x.latency)
              
              # 保底逻辑：至少保留30个
              current_count = len(merged)
              if current_count < MIN_NODES:
                  print(f"节点数不足{MIN_NODES}个，补充保底节点...")
                  need_add = MIN_NODES - current_count
                  for i in range(need_add):
                      dummy = ProxyNode({
                          "name": f"Backup_{random.randint(1000,9999)}",
                          "type": "trojan",
                          "server": f"backup{i}.example.com",
                          "port": 443,
                          "password": "dummy"
                      }, "HK-")
                      dummy.latency = 9999.0
                      merged.append(dummy)
              
              # 按地区限制最大节点数
              return merged[:max_nodes]

          def make_config(nodes, region_name):
              available_nodes = [n for n in nodes if n.available]
              if not available_nodes:
                  available_nodes = nodes[:MIN_NODES]
              
              names = [n.name for n in available_nodes]
              return {
                  "mixed-port": 7890,
                  "allow-lan": True,
                  "mode": "Rule",
                  "log-level": "info",
                  "external-controller": "127.0.0.1:9090",
                  "proxies": [n.to_dict() for n in available_nodes],
                  "proxy-groups": [
                      {
                          "name": f"{region_name} Auto",
                          "type": "url-test",
                          "proxies": names,
                          "url": "http://www.google.com/generate_204",
                          "interval": 300
                      },
                      {
                          "name": f"{region_name} Manual",
                          "type": "select",
                          "proxies": names + [f"{region_name} Auto", "DIRECT"]
                      }
                  ],
                  "rules": [
                      "GEOIP,CN,DIRECT",
                      f"MATCH,{region_name} Auto"
                  ]
              }

          def main():
              subs = load_subs()
              all_new_proxies = []
              for u in subs:
                  txt = get_content(u)
                  ps = parse_proxies(txt)
                  all_new_proxies.extend(ps)
              print(f"共获取新节点数: {len(all_new_proxies)}")

              for code, cfg in REGION_CONFIG.items():
                  print(f"\n===== 处理 {code.upper()} 节点 =====")
                  existing_nodes = load_existing_nodes(cfg["filename"])
                  print(f"历史节点数: {len(existing_nodes)}")

                  region_new_proxies = filter_region(all_new_proxies, cfg["keywords"])
                  new_nodes = [ProxyNode(p, cfg["prefix"]) for p in region_new_proxies]
                  print(f"新筛选节点数: {len(new_nodes)}")

                  print("开始内地视角测速...")
                  for i, n in enumerate(new_nodes):
                      print(f"测速中 {i+1}/{len(new_nodes)}: {n.name}")
                      n.test_latency_cf()
                      import time
                      time.sleep(0.2)

                  # 按地区取最大节点数
                  max_nodes = MAX_NODES_CONFIG.get(code, 30)
                  final_nodes = merge_nodes(existing_nodes, new_nodes, max_nodes)
                  print(f"最终保留节点数: {len(final_nodes)} (最大{max_nodes}个，保底{MIN_NODES}个)")

                  conf = make_config(final_nodes, code.upper())
                  with open(cfg["filename"], 'w', encoding='utf-8') as f:
                      yaml.dump(conf, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
                  print(f"{cfg['filename']} 已更新")

          if __name__ == "__main__":
              main()
          EOF

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add sub_*.yaml
          git commit -m "Auto update nodes (HK:100, SG/JP:30) with mainland latency $(date +'%Y-%m-%d %H:%M:%S')" || exit 0
          git push
